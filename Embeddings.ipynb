{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Глобальная идея здесь такая: я хочу построить эмбеддинги, которые помогут мне далее.\n",
    "1. Я напишу код для обучения эмбеддингов с нуля и поскладываю их как вектора, чтобы оченить их смысл.\n",
    "2. Я найду хорошие предобученные эмбеддинги\n",
    "3. Я найду датасет для классификации текстов и дообучу предобученные эмбеддинги на нем\n",
    "\n",
    "4. Сравню качество для задачи классификации на базовых предобученных эмбеддингах с качеством на дообученных эмбеддингах.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Итак, вспоминаем torch\n",
    "Для начала создадим датасет и все вспомогательные для работы с ним функции"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# TODO текст - последовательность токенов, так что сначала набо предобработать датасето, токнезироваав текст, добавив служебные символы и обрезав тексты\n",
    "class MoviesDataset(Dataset):\n",
    "    def __init__(self, path, max_len, train=True):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        full_df = pd.read_csv(path)[\"Plot\"]\n",
    "        processed_df = MoviesDataset.process_data(full_df, max_len)\n",
    "\n",
    "        train_df, test_df = train_test_split(processed_df, train_size=0.8, random_state=0)\n",
    "        if train:\n",
    "            self.data = train_df\n",
    "        else:\n",
    "            self.data = test_df\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data.copy()\n",
    "\n",
    "    @staticmethod\n",
    "    def process_data(data, max_len):\n",
    "        processed_data = []\n",
    "        slit_pattern = re.compile(r\"[,. 0-9()\\[\\]\\-\\\"\\'\\\\]\")\n",
    "        for text in data:\n",
    "            processed_text = list(filter(lambda token: token != '', re.split(slit_pattern, text.lower())))\n",
    "            if len(processed_text) < max_len:\n",
    "                processed_text.extend([\"<pad>\"] * (max_len - len(processed_text)))\n",
    "            else:\n",
    "                processed_text = processed_text[:max_len]\n",
    "            processed_data.append([\"<start>\"] + processed_text)\n",
    "        return processed_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "MAX_TEXT_LEN = 256\n",
    "SKIPGRAM_WINDOW_SIZE = 5\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 25\n",
    "MIN_TOKEN_FREQ = 5\n",
    "NUM_EPOCHS = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "movies_dataset_path = \"wiki_movie_plots_deduped.csv\"\n",
    "train_dataset = MoviesDataset(movies_dataset_path, train=True, max_len=MAX_TEXT_LEN)\n",
    "test_dataset = MoviesDataset(movies_dataset_path, train=False, max_len=MAX_TEXT_LEN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(train_dataset.get_data(), specials=[\"<unk>\"], min_freq=MIN_TOKEN_FREQ)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def my_embeddings_collate_fn(batch):\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    padding = int((SKIPGRAM_WINDOW_SIZE - 1)/2)\n",
    "    for text in batch:\n",
    "        text = text[:MAX_TEXT_LEN].split()\n",
    "        for window_start in range(len(text) - SKIPGRAM_WINDOW_SIZE + 1):\n",
    "            tokens = text[window_start:window_start + SKIPGRAM_WINDOW_SIZE]\n",
    "            x_current = tokens.pop(padding)\n",
    "\n",
    "            x_batch.extend([x_current] * (SKIPGRAM_WINDOW_SIZE - 1))\n",
    "            y_batch.extend(tokens)\n",
    "    return torch.tensor(x_batch, dtype=torch.int64), torch.tensor(y_batch, dtype=torch.int64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, collate_fn=my_embeddings_collate_fn, shuffle=True, batch_size=BATCH_SIZE, num_workers=6)\n",
    "test_loader = DataLoader(test_dataset, collate_fn=my_embeddings_collate_fn, shuffle=True, batch_size=BATCH_SIZE, num_workers=6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class Word2VecModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.linear = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "model = Word2VecModel(vocab_size=len(vocab), embedding_dim=EMBEDDING_DIM)\n",
    "optimizer = torch.optim.Adam(params=model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def plot_losses(train_history, valid_history):\n",
    "    clear_output()\n",
    "    plt.plot(train_history, label='train', color='green')\n",
    "    plt.plot(valid_history, label='valid', color='red')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def training_epoch(model, optimizer, criterion, train_loader, tqdm_desc):\n",
    "    cumulative_loss = 0\n",
    "    model.train()\n",
    "    for inputs, outputs in tqdm(train_loader, desc=tqdm_desc):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = outputs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cumulative_loss += loss.item()\n",
    "\n",
    "    return cumulative_loss / len(train_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_epoch(model, criterion, valid_loader, tqdm_desc):\n",
    "    cumulative_loss = 0\n",
    "    model.eval()\n",
    "    for inputs, outputs in tqdm(valid_loader, desc=tqdm_desc):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = outputs.to(device)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, outputs)\n",
    "\n",
    "        cumulative_loss += loss.item()\n",
    "    return cumulative_loss / len(valid_loader)\n",
    "\n",
    "\n",
    "def train(train_loader, valid_loader, num_epochs, model, optimizer, criterion, scheduler=None):\n",
    "    train_history, valid_history = [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = training_epoch(\n",
    "            model, optimizer, criterion, train_loader,\n",
    "            tqdm_desc=f'Training {epoch}/{num_epochs}'\n",
    "        )\n",
    "        valid_loss = validation_epoch(\n",
    "            model, criterion, valid_loader,\n",
    "            tqdm_desc=f'Validating {epoch}/{num_epochs}'\n",
    "        )\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_history.append(train_loss)\n",
    "        valid_history.append(valid_loss)\n",
    "        plot_losses(train_history, valid_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1/10:   0%|          | 0/873 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'MoviesDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Training 1/10:   0%|          | 0/873 [03:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train(\n\u001B[1;32m      2\u001B[0m     train_loader,\n\u001B[1;32m      3\u001B[0m     test_loader,\n\u001B[1;32m      4\u001B[0m     NUM_EPOCHS,\n\u001B[1;32m      5\u001B[0m     model,\n\u001B[1;32m      6\u001B[0m     optimizer,\n\u001B[1;32m      7\u001B[0m     criterion\n\u001B[1;32m      8\u001B[0m )\n",
      "Cell \u001B[0;32mIn[28], line 46\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(train_loader, valid_loader, num_epochs, model, optimizer, criterion, scheduler)\u001B[0m\n\u001B[1;32m     43\u001B[0m train_history, valid_history \u001B[38;5;241m=\u001B[39m [], []\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, num_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m---> 46\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m training_epoch(\n\u001B[1;32m     47\u001B[0m         model, optimizer, criterion, train_loader,\n\u001B[1;32m     48\u001B[0m         tqdm_desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     49\u001B[0m     )\n\u001B[1;32m     50\u001B[0m     valid_loss \u001B[38;5;241m=\u001B[39m validation_epoch(\n\u001B[1;32m     51\u001B[0m         model, criterion, valid_loader,\n\u001B[1;32m     52\u001B[0m         tqdm_desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mValidating \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     53\u001B[0m     )\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m scheduler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "Cell \u001B[0;32mIn[28], line 12\u001B[0m, in \u001B[0;36mtraining_epoch\u001B[0;34m(model, optimizer, criterion, train_loader, tqdm_desc)\u001B[0m\n\u001B[1;32m     10\u001B[0m cumulative_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     11\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, outputs \u001B[38;5;129;01min\u001B[39;00m tqdm(train_loader, desc\u001B[38;5;241m=\u001B[39mtqdm_desc):\n\u001B[1;32m     13\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     14\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/tqdm/std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1182\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:439\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    437\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 439\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_iterator()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:387\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    385\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_worker_number_rationality()\n\u001B[0;32m--> 387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _MultiProcessingDataLoaderIter(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1040\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[0;34m(self, loader)\u001B[0m\n\u001B[1;32m   1033\u001B[0m w\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[1;32m   1036\u001B[0m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[1;32m   1038\u001B[0m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[0;32m-> 1040\u001B[0m w\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m   1041\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_queues\u001B[38;5;241m.\u001B[39mappend(index_queue)\n\u001B[1;32m   1042\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workers\u001B[38;5;241m.\u001B[39mappend(w)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemon\u001B[39m\u001B[38;5;124m'\u001B[39m), \\\n\u001B[1;32m    119\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemonic processes are not allowed to have children\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    120\u001B[0m _cleanup()\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Popen(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sentinel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen\u001B[38;5;241m.\u001B[39msentinel\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[0;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_context\u001B[38;5;241m.\u001B[39mget_context()\u001B[38;5;241m.\u001B[39mProcess\u001B[38;5;241m.\u001B[39m_Popen(process_obj)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/context.py:289\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[1;32m    288\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopen_spawn_posix\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Popen(process_obj)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, process_obj):\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fds \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 32\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(process_obj)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/popen_fork.py:19\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinalizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_launch(process_obj)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001B[0m, in \u001B[0;36mPopen._launch\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentinel \u001B[38;5;241m=\u001B[39m parent_r\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(parent_w, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m, closefd\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m---> 62\u001B[0m         f\u001B[38;5;241m.\u001B[39mwrite(fp\u001B[38;5;241m.\u001B[39mgetbuffer())\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m     fds_to_close \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    NUM_EPOCHS,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
